{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickAI2811/DataToolKit/blob/main/Stable_Diffusion_LNMIIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic:** Stable Diffusion on Medical Datasets\n",
        "\n",
        "**Agenda:**\n",
        "1.  A gentle introduction to the stable diffusion process (for beginners)\n",
        "2.  Implementation of stable diffusion on MEDMNIST dataset using Pytorch\n",
        "\n",
        "\n",
        "**Diffusion Models**\n",
        "1. Emerging class of generative networks; driven by thermodynamics\n",
        "2. Generic pipeline has three stages: Forward process, Reverse process and Sampling process\n",
        "\n",
        "**Why Stable Diffusion ?**\n",
        "1. Operations (of forward, reverse and sampling steps) done on rich representations in the latent space\n",
        "2. Consequently, training and sampling remains reliable, without divergence, mode collapse and extreme computation costs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "poSMMX7rZj8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwqRPUD1YX8L"
      },
      "outputs": [],
      "source": [
        "#1 Install necessary packages\n",
        "\n",
        "!pip install torch                                       # the core deep learning engine (tensors, autograd, neural networks, GPU support)\n",
        "!pip install torchvision                                 # computer-vision add-on for PyTorch (datasets, image transforms, pretrained vision models)\n",
        "!pip install medmnist                                    # to download medmnist dataset\n",
        "!pip install tqdm                                        # to display live progress bars for loops (training, data loading, downloads, etc.)\n",
        "!pip install pillow                                      # for loading, saving, and preprocessing images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Import necessary packages\n",
        "\n",
        "import torch\n",
        "torch.set_printoptions(sci_mode=False, precision=4)                 # Setting printoptions, optional step\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(suppress=True, precision=4)                     # Setting printoptions, optional step\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, utils                           # mostly focused on visualization, image manipulation, and debugging\n",
        "from medmnist import INFO                                           # a dictionary (with dataset names as keys) object to store the metadata\n",
        "from tqdm import tqdm                                               # package to visualize the progress\n",
        "import os\n",
        "import random\n",
        "import medmnist"
      ],
      "metadata": {
        "id": "TyEzB1MxYhQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Create a separate directory for the results\n",
        "\n",
        "SAVE_DIR = \"./results\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Setting the seed values\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "XVdmQFsgYjU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Setting hyperparameters and computation device\n",
        "\n",
        "TIMESTEPS = 1000                                                                # total number of discrete steps in the forward and reverse process.\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")            # Set-up the computation device\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.00001\n",
        "EPOCHS = 2"
      ],
      "metadata": {
        "id": "4RSKWHW-Y8yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Load the MNIST ChestMNIST dataset\n",
        "\n",
        "info = INFO['chestmnist']\n",
        "DataClass = getattr(medmnist, info['python_class'])            # dynamically fetches the dataset class from the MedMNIST module using the class name"
      ],
      "metadata": {
        "id": "5jSUz7IgY2uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Define the transformations to be applied to the image; additional transformations may be explored as a homework\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                       # convert to tensor\n",
        "    transforms.Normalize([0.5], [0.5])           # normalize to mean = 0.5, std = 0.5\n",
        "])"
      ],
      "metadata": {
        "id": "deccEPdfY-pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Get the training dataset and initialize the trainloader\n",
        "\n",
        "train_dataset = DataClass(split='train', transform=transform, download=True)\n",
        "print(\"type(train_dataset): \", type(train_dataset), \"\\n\")\n",
        "print(\"train_dataset: \\n\\n\", train_dataset, \"\\n\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "RzfTlc_aZA9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Exploring a single batch of scans\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "images = images.squeeze().numpy()\n",
        "labels = labels.squeeze().numpy()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "subset = images[0:16]                                                               # plot first 16 images\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(subset[i].squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k84ug94eZFRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generic Pipeline of Stable Diffusion**\n",
        "\n",
        "1. Encoder obtains the latent embeddings $z$ of input image $x$ at timestep\n",
        "\\begin{equation}\n",
        "x → Encoder → z\n",
        "\\end{equation}\n",
        "2. Forward Process:\n",
        "\\begin{equation}\n",
        "z_{t=0} → Forward → z_{t=T}\n",
        "\\end{equation}\n",
        "3. Reverse (Sampling) Process:\n",
        "\\begin{equation}\n",
        "z_{t=T} → Reverse (Sampling)→ \\hat{z}_{t=0}\n",
        "\\end{equation}\n",
        "4. Decoder reconstructs the input image $x$ as\n",
        "\\begin{equation}\n",
        "\\hat{z} → Decoder → \\hat{x}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "y1pkT3XZ0gVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the encoder and decoder network to obtain the latent embeddings (stable diffusion works on latent space)\n",
        "# Encoder and Decoder can also be one of the existing networks such as ResNet etc.\n",
        "\n",
        "# original image (x) ---Encoder--> Embeddings (z) ---Decoder--> Reconstructed Image (x')\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim=4):                                            # Setting the latent channels to be 4\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, latent_dim, 4, 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)                                                        # shape of latent embeddings --> (batch_size, 4, 7, 7)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 32, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)                                                       # shape of the reconstructed images --> (batch_size, 1, 28, 28)"
      ],
      "metadata": {
        "id": "Fs74bI6gaKPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the model summary of the encoder-decoder networks\n",
        "\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "latent_dim = 4                                                                   # Number of latent channels in the encoder-decoder network\n",
        "\n",
        "encoder = Encoder(latent_dim).to(DEVICE)\n",
        "decoder = Decoder(latent_dim).to(DEVICE)\n",
        "\n",
        "print(\"Encoder Summary:\")\n",
        "summary(encoder, input_size=(1, 28, 28))                                         # (batch_size, 1, 28, 28) ---Encoder--> (batch_size, 4, 7, 7)\n",
        "\n",
        "print(\"Decoder Summary:\")\n",
        "summary(decoder, input_size=(latent_dim, 7, 7))                                  # (batch_size, 4, 7, 7) ---Decoder--> (batch_size, 1, 28, 28)"
      ],
      "metadata": {
        "id": "H8-PG2yYkuzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrain the encoder and decoder networks on the original dataset images (x)\n",
        "\n",
        "encoder = Encoder(latent_dim=4).to(DEVICE)\n",
        "decoder = Decoder(latent_dim=4).to(DEVICE)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(encoder.parameters()) + list(decoder.parameters()),\n",
        "    lr=1e-3\n",
        ")\n",
        "\n",
        "ED_EPOCHS = 1\n",
        "\n",
        "for epoch in range(ED_EPOCHS):\n",
        "    total_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{ED_EPOCHS}]\", leave=True)\n",
        "\n",
        "    for x, _ in pbar:  # x in [-1, 1]\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        z = encoder(x)\n",
        "        x_hat = decoder(z)\n",
        "\n",
        "        loss = criterion(x_hat, x)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        pbar.set_postfix(AE_Loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{ED_EPOCHS}] | Avg AE Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "LLbUQSlqQKNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the encoder and decoder network weights\n",
        "\n",
        "torch.save(encoder.state_dict(), \"encoder_pretrained.pth\")\n",
        "torch.save(decoder.state_dict(), \"decoder_pretrained.pth\")"
      ],
      "metadata": {
        "id": "NVJ3KHaSSBOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the encoder-decoder network\n",
        "\n",
        "encoder.load_state_dict(torch.load(\"encoder_pretrained.pth\"))\n",
        "decoder.load_state_dict(torch.load(\"decoder_pretrained.pth\"))\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "for p in decoder.parameters():\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "vu4p9MM9SI_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward Process**\n",
        "\n",
        "1.  The forward diffusion process perturbs a latent representation $z_0$ to $z_{t=1}^{T}$ as timesteps progress.\n",
        "2.  A forward transition $p(z_t \\mid z_{t-1})$ describes this perturbation, where $\\epsilon_t$ is the noise added at timestep $(t)$.\n",
        "\n",
        "\\begin{equation}\n",
        "p(z_T \\mid z_0)\n",
        "= p(z_1 \\mid z_0)\\; \\dots\\; p(z_t \\mid z_{t-1})\\; \\dots\\; p(z_T \\mid z_{T-1})\n",
        "= \\prod_{t=1}^{T} p(z_t \\mid z_{t-1})\n",
        "\\end{equation}\n",
        "\n",
        "3.  Through multiple timesteps, the original latent distribution $p(z_0)$ is eventually perturbed to a tractable terminal distribution $p(z_T)$.\n",
        "\n",
        "\n",
        "**Implementation of Forward Process**\n",
        "\n",
        "1. Typically, stable diffusion models are implemented as Denoising Diffusion Probabilistic Models (DDPMs).\n",
        "2. The forward process gradually adds Gaussian noise to an image over $T$ timesteps and therefore, are well-grounded probabilistically:\n",
        "\n",
        "\\begin{equation}\n",
        "p(z_t \\mid z_{t-1}) =\n",
        "\\mathcal{N}\\Big(\n",
        "z_t;\\; \\sqrt{1-\\beta_t}\\, z_{t-1},\\; \\beta_t I\n",
        "\\Big)\n",
        "\\end{equation}\n",
        "\n",
        "where:   \n",
        "*   $z_t$ is noisy latent representation at timestep $(t)$\n",
        "*   $\\beta_t$ is noise variance added at step $(t)$\n",
        "\n",
        "3.  The above conditional distribution implies that $z_{t}$ can be sampled as\n",
        "\n",
        "\\begin{equation}\n",
        "z_t\n",
        "=\n",
        "\\sqrt{1-\\beta_t}\\, z_{t-1}\n",
        "+\n",
        "\\sqrt{\\beta_t}\\, \\epsilon_t,\n",
        "\\qquad\n",
        "\\epsilon_t \\sim \\mathcal{N}(0, I)\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "*  $\\sqrt{1-\\beta_t}\\, z_{t-1}$ is the deterministic / mean component\n",
        "*  $\\sqrt{\\beta_t}\\, \\epsilon_t$ is the noise\n",
        "*  $\\Sigma_t = \\beta_t I$ is the covariance\n",
        "4. Stable diffusion uses a cosine noise schedule $\\beta_{t}$ given by\n",
        "\\begin{equation}\n",
        "\\beta_t\n",
        "=\n",
        "\\beta_{\\min}\n",
        "+\n",
        "\\frac{1}{2}\n",
        "\\left(\\beta_{\\max} - \\beta_{\\min}\\right)\n",
        "\\left(\n",
        "1 - \\cos\\left(\\frac{\\pi\\, t}{T - 1}\\right)\n",
        "\\right),\n",
        "\\quad t = 0, 1, \\dots, T-1\n",
        "\\end{equation}\n",
        "where $\\beta_{max}$ and $\\beta_{min}$ are hyperparameters\n",
        "5. Fixed and non-learnable; carefully chosen learning schedules balance exploding noise and vanishing signal"
      ],
      "metadata": {
        "id": "hb9J5PIOZqPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Setting the noise variance to be added at each time step.\n",
        "\n",
        "import math\n",
        "\n",
        "def cosine_beta_schedule(T, beta_min=1e-4, beta_max=2e-2):\n",
        "    t = torch.linspace(0, T - 1, T)\n",
        "    betas = beta_min + 0.5 * (beta_max - beta_min) * (1 - torch.cos(math.pi * t / (T - 1)))\n",
        "    return betas\n",
        "\n",
        "# Computing the noise variance (beta) values\n",
        "\n",
        "betas = cosine_beta_schedule(TIMESTEPS).to(DEVICE)\n",
        "print(\"betas.size(): \\t\", betas.size(), \"\\n\")\n",
        "\n",
        "# Plotting noise variance (beta) values against time-steps\n",
        "\n",
        "x_values = np.linspace(1,TIMESTEPS,TIMESTEPS)\n",
        "y_values = betas\n",
        "y_values = y_values.cpu()\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x_values, y_values)\n",
        "plt.xlabel(\"Number of time steps\")\n",
        "plt.ylabel(\"Noise variance (beta) added\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Variation of noise variance (beta) against time-steps\", y=-0.20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x3gseHytYnUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define:\n",
        "\n",
        "\\begin{equation}\n",
        "\\alpha_t = 1 - \\beta_t\n",
        "\\end{equation}\n",
        "\n",
        "where:  \n",
        "\n",
        "*  $\\beta_t$ is the variance of the Gaussian noise added at timestep $(t)$\n",
        "*  $\\alpha_t$ represents the fraction of the original signal retained at that timestep $(t)$\n",
        "\n",
        "Using the above substitution, the forward process can be written as:\n",
        "\n",
        "\\begin{equation}\n",
        "z_t = \\sqrt{\\alpha_t} \\, z_{t-1} + \\sqrt{\\beta_t} \\, \\epsilon_{t}, \\quad \\epsilon_{t} \\sim \\mathcal{N}(0,I)\n",
        "\\end{equation}\n",
        "\n",
        "Impact of $\\alpha_{t}$ on the noising process:\n",
        "1.   Early timesteps: $\\alpha_t \\approx 1$ → most of the latent representation is preserved\n",
        "2.   Later timesteps:  $\\alpha_t < 1$ → more noise is added\n",
        "\n"
      ],
      "metadata": {
        "id": "121iD3BPZxqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#11 Computing alpha values from beta values\n",
        "\n",
        "alphas = 1 - betas\n",
        "print(\"alphas.size(): \\t\", alphas.size(), \"\\n\")\n",
        "\n",
        "# Plotting noise variance (alpha) values against time-steps\n",
        "\n",
        "x_values = np.linspace(1,TIMESTEPS,TIMESTEPS)\n",
        "y_values = alphas\n",
        "y_values = y_values.cpu()\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x_values, y_values)\n",
        "plt.xlabel(\"Number of time steps\")\n",
        "plt.ylabel(\"Noise variance (alpha) added\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Variation of noise variance (alpha) against time-steps\", y=-0.20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8hYxoB17YyA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cumulative product of alphas gives the fraction of the original latent representation remaining after $(t)$ steps:\n",
        "\n",
        "\\begin{equation}\n",
        "\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\n",
        "\\end{equation}\n",
        "\n",
        "Then, $z_t$ can be sampled directly from $z_0$ as:\n",
        "\n",
        "\\begin{equation}\n",
        "z_t = \\sqrt{\\bar{\\alpha}_t} \\, z_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon_{t}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "1hfgQtpsZ1de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#12 Redefining noise variance added at each step\n",
        "\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)                           # Returns the cumulative product of elements in the input vector\n",
        "\n",
        "print(\"alphas_cumprod.size(): \\t\", alphas_cumprod.size(), \"\\n\")\n",
        "\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)"
      ],
      "metadata": {
        "id": "JFdntMAtY0hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13 Generate a noisy latent embedding at timestep t using the forward diffusion formula\n",
        "\n",
        "def q_sample(z, t, noise):\n",
        "    sqrt_a = sqrt_alphas_cumprod[t].view(-1,1,1,1)\n",
        "    sqrt_one_minus_a = sqrt_one_minus_alphas_cumprod[t].view(-1,1,1,1)\n",
        "    return sqrt_a * z + sqrt_one_minus_a * noise"
      ],
      "metadata": {
        "id": "RoYJbHFfZcWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of the Reverse Process**\n",
        "\n",
        "**Reverse Process**\n",
        "1.  Trains a denoising network to remove the noise added in the forward process\n",
        "2.  Specifically, the reverse process moves on the chain in the opposite direction and iteratively removes noise between two consecutive timesteps as $(t)$ decreases from $T$ to $0$\n",
        "\n",
        "\\begin{equation}\n",
        "p_{\\theta}(z_{0})\n",
        "= p_{\\theta}(z_{T})\\, p_{\\theta}(z_{T-1}\\mid z_{T}) \\;\\dots\\; p_{\\theta}(z_{t-1}\\mid z_{t}) \\;\\dots\\; p_{\\theta}(z_{0}\\mid _{1}) \\\\\n",
        "= p_{\\theta}(z_{T}) \\prod_{t=1}^{T} p_{\\theta}(z_{t-1}\\mid z_{t})\n",
        "\\end{equation}\n",
        "\n",
        "3.  The reverse process in the above step is modelled as\n",
        "\\begin{equation}\n",
        " p_{\\theta}(z_{t-1}|z_{t}) = \\mathcal{N}(z_{t-1}; \\mu_{\\theta}(z_{t},t), \\Sigma_{\\theta}(z_{t},t))\n",
        " \\end{equation}\n",
        "\n",
        "In this implementation, UNet is used as the denoising network\n",
        "\n",
        "1. UNet combined multiscale information through skip connections; denoising becomes easy as the network can propagate low-level and high-level features\n",
        "2. UNet is parameterized using $\\theta$ described above.\n",
        "\n"
      ],
      "metadata": {
        "id": "e5UA24AbfBes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#14 Model the noise added in the forward process using UNet architecture; UNet acts as a denoising model\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, in_ch=latent_dim, base_ch=64, time_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(1, time_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_dim, time_dim)                    # maps a scalar diffusion timestep into a high-dimensional learned embedding\n",
        "        )\n",
        "\n",
        "        self.enc1 = Block(in_ch, base_ch, time_dim)          # Encoder blocks inject time information to condition feature extraction\n",
        "        self.enc2 = Block(base_ch, base_ch * 2, time_dim)\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "\n",
        "        self.bot = Block(base_ch * 2, base_ch * 2, time_dim) # captures the richest representations (global structure + time step information)\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
        "        self.dec2 = Block(base_ch * 4, base_ch, time_dim)\n",
        "        self.dec1 = Block(base_ch * 2, base_ch, time_dim)    # decoder blocks use the same time conditioning to guide noise removal at that timestep\n",
        "\n",
        "        self.out = nn.Conv2d(base_ch, in_ch, 1)\n",
        "\n",
        "    def forward(self, x, t):                                 # # Input: embeddings (batch_size, latent_dim, H, W) and t (batch_size, 1)\n",
        "\n",
        "        t = self.time_mlp(t.view(-1, 1))                     # (batch_size, 1) --> (batch_size, time_dim)\n",
        "\n",
        "        e1 = self.enc1(x, t)                                 # (batch_size, latent_dim, H, W) --> (batch_size, base_ch, H, W)\n",
        "        e2 = self.enc2(self.pool(e1), t)                     # (batch_size, base_ch, H, W) --> (batch_size, base_ch, H/2, W/2) --> (batch_size, base_ch*2, H/2, W/2)\n",
        "\n",
        "        b = self.bot(self.pool(e2), t)                       # (batch_size, base_ch*2, H/2, W/2) --> (batch_size, base_ch*2, H/4, W/4) --> (batch_size, base_ch*2, H/4, W/4)\n",
        "\n",
        "        d2 = self.dec2(torch.cat([F.interpolate(b, size=e2.shape[-2:], mode='nearest'), e2], dim=1), t)  # (batch_size, base_ch*2, H/4, W/4) --> (batch_size, base_ch*2, H/2, W/2) --> (batch_size, base_ch, H/2, W/2)\n",
        "        d1 = self.dec1(torch.cat([F.interpolate(d2, size=e1.shape[-2:], mode='nearest'), e1], dim=1), t) # (batch_size, base_ch, H/2, W/2) --> (batch_size, base_ch, H, W) --> (batch_size, base_ch, H, W)\n",
        "\n",
        "        return self.out(d1) # (batch_size, base_ch, H, W) --> (batch_size, latent_dim, H, W)\n"
      ],
      "metadata": {
        "id": "Rh-JftpkZOvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time-Injection**\n",
        "\n",
        "1.    Time injection is the process of feeding the timestep $(t)$ into the denoising network so it knows the noise level at that stage of the reverse diffusion process.\n",
        "2.    Without this, the model cannot distinguish early noisy steps from later ones; reverse diffusion becomes unstable and inconsistent\n",
        "3.  Can be chosen freely; larger values give richer embeddings but increase the number of parameters in the Linear layer.\n"
      ],
      "metadata": {
        "id": "J7N_mx_7ZK_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inject timestep information into every feature, letting the network know which diffusion step it is processing, without adding extra noise.\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.time = nn.Linear(time_dim, out_ch)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        h = self.act(self.conv1(x))                       # (batch_size, in_ch, H, W) --> (batch_size, out_ch, H, W)\n",
        "        h = h + self.time(t).unsqueeze(-1).unsqueeze(-1)  # (batch_size, time_dim) --> (batch_size, out_ch) --> (batch_size, out_ch, 1, 1) --> (batch_size, out_ch, H, W)\n",
        "        h = self.act(self.conv2(h))                       # (batch_size, out_ch, H, W) --> (batch_size, out_ch, H, W)\n",
        "        return h"
      ],
      "metadata": {
        "id": "oOFsf681ZMDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model summary for the above UNET model\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "latent_size = 7                                   # spatial dimension of latent tensor fed into SimpleUnet;  (4, 7, 7) --> (4, 7, 7)\n",
        "time_dim = 128\n",
        "\n",
        "model = SimpleUNet(\n",
        "    in_ch=latent_dim,\n",
        "    base_ch=64,\n",
        "    time_dim=128\n",
        ").to(DEVICE)\n",
        "\n",
        "class ModelWrapper(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        t = torch.zeros(x.size(0), 1, device=x.device)\n",
        "        return self.model(x, t)\n",
        "\n",
        "wrapped_model = ModelWrapper(model).to(DEVICE)\n",
        "\n",
        "print(\n",
        "    summary(\n",
        "        wrapped_model,\n",
        "        input_size=(latent_dim, latent_size, latent_size),\n",
        "        device=str(DEVICE)\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "YVaCuJTqZUq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Sampling Process**\n",
        " 1.  Leverages the optimized denoising network parameters $\\theta^{*}$ to generate novel latent embeddings $\\hat{z}_{0}$.\n",
        "2. Sampling obtains a latent sample $z_{T}$ from the terminal distribution $p(z_{T})$ and then uses the trained network to iteratively remove noise according to the transition $p_{\\theta^{*}}(z_{t-1}\\mid z_{t})$.\n",
        "\n",
        "\\begin{equation}\n",
        "p_{\\theta^{*}}(z_{0}) = p_{\\theta^{*}}(z_{T})\\,p_{\\theta^{*}}(z_{T-1}\\mid z_{T}) \\cdots p_{\\theta^{*}}(z_{0}\\mid z_{1})\n",
        "= p_{\\theta^{*}}(z_{T}) \\prod_{t=1}^{T} p_{\\theta^{*}}(z_{t-1}\\mid z_{t})\n",
        "\\end{equation}\n",
        "\n",
        "**Steps**\n",
        "\n",
        "1. Start from a pure Gaussian noise in latent space $z_T$.  \n",
        "2. At each step, remove the predicted noise using the UNet and rescale the latent embedding.  \n",
        "3. Repeat until $t = 0$, producing a clean latent representation $\\hat{z}_0$, which can then be decoded into the image space.\n",
        "\n",
        "\\begin{equation}\n",
        "z_{t-1}\n",
        "=\n",
        "\\frac{1}{\\sqrt{\\alpha_t}}\n",
        "\\left(\n",
        "z_t\n",
        "-\n",
        "\\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\n",
        "\\,\\hat{\\epsilon}_\\theta(z_t, t)\n",
        "\\right)\n",
        "+\n",
        "\\mathbf{1}_{t>0}\\,\\sqrt{\\beta_t}\\,\\epsilon,\n",
        "\\qquad\n",
        "\\epsilon \\sim \\mathcal{N}(0, I)\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "4iiMUYF8Zbpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#17 Sampling process; Reverse Diffusion (Latent Sampling)\n",
        "\n",
        "@torch.no_grad()                                                                   # no gradients are computed (inference mode)\n",
        "\n",
        "def sample_latent(model, decoder, n_samples=16):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    z = torch.randn(n_samples, 4, 7, 7).to(DEVICE)                                # z_T in the DDPM notation\n",
        "\n",
        "    for t in reversed(range(TIMESTEPS)):                                          # Iterates backwards through timesteps from T-1 to 0.\n",
        "        t_batch = torch.full((n_samples,), t, device=DEVICE).float() / TIMESTEPS\n",
        "        pred_noise = model(z, t_batch)                                            # predicted noise component in z at timestep t\n",
        "\n",
        "        alpha = alphas[t]                                                         # Core reverse diffusion step\n",
        "        alpha_bar = alphas_cumprod[t]\n",
        "        beta = betas[t]\n",
        "        z = (1 / torch.sqrt(alpha)) * (z - beta * pred_noise / torch.sqrt(1 - alpha_bar))  # remove predicted noise from current latent z\n",
        "\n",
        "        if t > 0:\n",
        "            z += torch.sqrt(beta) * torch.randn_like(z)\n",
        "\n",
        "    x = decoder(z)                                                                # map from latent to original feature/image space\n",
        "    return x.clamp(-1,1)"
      ],
      "metadata": {
        "id": "2jNLdC6IZeQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Initializing the models' hyperparameters and setting the loss function\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "encoder = Encoder().to(DEVICE)\n",
        "decoder = Decoder().to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "IEvBqCwcZW-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19 Train the denoising model (UNet) in the reverse direction\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    pbar = tqdm(train_loader)\n",
        "    for x, _ in pbar:\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        z = encoder(x)\n",
        "        noise = torch.randn_like(z)\n",
        "\n",
        "        t = torch.randint(0, TIMESTEPS, (z.size(0),), device=DEVICE)  # Each sample gets a different random timestep, so network sees all timesteps\n",
        "        z_t = q_sample(z, t, noise)    # t determines alpha values\n",
        "        t_norm = t.float() / TIMESTEPS  # networks process inputs in a small, continuous range better\n",
        "\n",
        "        pred_noise = model(z_t, t_norm)\n",
        "        loss = loss_fn(pred_noise, noise)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    samples = sample_latent(model, decoder, 16)\n",
        "    samples = (samples + 1) / 2\n",
        "    utils.save_image(samples, f\"{SAVE_DIR}/epoch_{epoch+1}.png\", nrow=4)"
      ],
      "metadata": {
        "id": "FVOYd0KiZYgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and the artificial samples as Pytorch tensors\n",
        "\n",
        "torch.save(model.state_dict(), f\"{SAVE_DIR}/ddpm_chestmnist.pt\")\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "wSoeMTaiZg4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}